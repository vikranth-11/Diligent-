ğŸ¤– Jarvis â€” Local AI Assistant with RAG

Jarvis is a local, enterprise-oriented AI assistant built using a local LLM (Ollama) and Retrieval-Augmented Generation (RAG).
It answers user questions by retrieving relevant information from local documents and generating responses using a locally running language model.

This project is designed to be:

ğŸ” Fully local (no cloud APIs required)

ğŸ§  Context-aware using vector search

ğŸ’¬ ChatGPT-style UI

ğŸ¢ Enterprise-focused

âœ¨ Key Features

Local LLM inference using Ollama (Mistral)

RAG pipeline with FAISS vector database

Semantic search using Sentence Transformers

FastAPI backend

Streamlit ChatGPT-style frontend

Multiple conversations with sidebar navigation

Clear scope control via system prompt

All data stays on your machine

ğŸ§± Project Structure
jarvis-ai/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app.py # FastAPI backend
â”‚ â”‚
â”‚ â”œâ”€â”€ llm/
â”‚ â”‚ â”œâ”€â”€ mock_llm.py # Mock LLM (early development)
â”‚ â”‚ â””â”€â”€ local_llm.py # Ollama integration (Mistral)
â”‚ â”‚
â”‚ â”œâ”€â”€ rag/
â”‚ â”‚ â”œâ”€â”€ ingest.py # Document ingestion & indexing
â”‚ â”‚ â””â”€â”€ vector_store.py # FAISS vector store
â”‚ â”‚
â”‚ â”œâ”€â”€ prompts/
â”‚ â”‚ â””â”€â”€ system.txt # System prompt (behavior control)
â”‚ â”‚
â”‚ â””â”€â”€ **init**.py
â”‚
â”œâ”€â”€ ui/
â”‚ â””â”€â”€ app.py # Streamlit UI (ChatGPT-style)
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ sample_docs.txt # Knowledge base documents
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ—ï¸ Architecture Diagram
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Streamlit UI â”‚
â”‚ (ChatGPT-style frontend) â”‚
â”‚ ui/app.py â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ HTTP (POST /chat)
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FastAPI Backend â”‚
â”‚ backend/app.py â”‚
â”‚ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ System Prompt â”‚ â”‚
â”‚ â”‚ backend/prompts/ â”‚ â”‚
â”‚ â”‚ system.txt â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”‚ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ RAG Pipeline â”‚ â”‚
â”‚ â”‚ backend/rag/ â”‚ â”‚
â”‚ â”‚ - ingest.py â”‚ â”‚
â”‚ â”‚ - vector_store.py â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”‚ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ FAISS Vector Database â”‚ â”‚
â”‚ â”‚ (local, in-memory) â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”‚ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Local LLM (Ollama) â”‚ â”‚
â”‚ â”‚ Model: Mistral â”‚ â”‚
â”‚ â”‚ backend/llm/local_llm â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§  How It Works (Flow)

User enters a message in the Streamlit UI

UI sends request to FastAPI /chat endpoint

Backend:

Retrieves relevant document chunks using FAISS

Injects context + system prompt into final prompt

Prompt is sent to Ollama (Mistral) running locally

Model generates a response

Response is returned and rendered in the UI

âš™ï¸ Setup Instructions
1ï¸âƒ£ Prerequisites

Python 3.10+

Git

Ollama installed

ğŸ‘‰ Install Ollama:
https://ollama.com/download

2ï¸âƒ£ Clone the Repository
git clone https://github.com/your-username/jarvis-ai.git
cd jarvis-ai

3ï¸âƒ£ Create & Activate Virtual Environment
python -m venv venv
venv\Scripts\activate # Windows

# source venv/bin/activate # Linux / macOS

4ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

5ï¸âƒ£ Pull the LLM Model
ollama pull mistral

Verify:

ollama run mistral

6ï¸âƒ£ Start the Backend
uvicorn backend.app:app --reload

Backend runs at:

http://127.0.0.1:8000

Swagger UI:

http://127.0.0.1:8000/docs

7ï¸âƒ£ Start the UI

In a new terminal:

streamlit run ui/app.py

UI runs at:

http://localhost:8501

Example Queries

âœ… In-scope:

â€œExplain our system architectureâ€

â€œWhat is the purpose of this assistant?â€

âš ï¸ Out-of-scope:

â€œTeach me alphabetsâ€

â€œWrite a poemâ€

The assistant clearly communicates scope limitations and may optionally provide general reference information depending on prompt configuration.

ğŸ” Design Principles

Local-first: No external APIs

Privacy-preserving: Data never leaves the machine

Scope-aware: Optimized for enterprise knowledge use

Modular: LLM, RAG, UI are decoupled

Extensible: Easy to add new documents or models

ğŸ Conclusion

Jarvis demonstrates a production-grade local AI assistant architecture using modern RAG techniques, local inference, and a polished chat UI â€” without relying on any cloud LLM APIs.
